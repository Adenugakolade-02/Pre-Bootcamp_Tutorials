{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will provide explanation on the concept of hyperparameter optimization. We will have to rely on the previous classes notebook on feature engineering and cross validation.  You can check the github repository for those contents.\n",
    "I'll skip the data  pre-preprocessing and feature engineering steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import skew\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train.drop(\"SalePrice\", axis=1),test], axis=0) #Join train and test dataset\n",
    "y = train[['SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop features with high missing values(over 80%)\n",
    "high_missing_cols = ['PoolQC', 'MiscFeature', 'Alley', 'Fence',]\n",
    "data = data.drop(high_missing_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll drop some features due to multicollinearity, low class representation, etc\n",
    "to_drop = ['Id', 'YrSold', 'MoSold', 'Utilities', 'Street', 'Condition2', 'RoofMatl', 'Heating',\n",
    "           'LowQualFinSF', '3SsnPorch', 'PoolArea', 'MiscVal']\n",
    "data = data.drop(to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope',\n",
       "       'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle',\n",
       "       'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond',\n",
       "       'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
       "       'BsmtFinType2', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n",
       "       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get list of categorical features\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
       "       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
       "       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n",
       "       'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
       "       'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt',\n",
       "       'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', 'ScreenPorch'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of numeric columns\n",
    "numeric_cols = data.select_dtypes(include=np.number).columns\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace NaN with none.\n",
    "none_cols = ['FireplaceQu', 'GarageType','GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual',\n",
    "             'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType']\n",
    "for col in none_cols:\n",
    "    data[col].replace(np.nan, 'None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill missing categorical columns with the mode\n",
    "data[categorical_cols] = data[categorical_cols].fillna(data[categorical_cols].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imputer = IterativeImputer()\n",
    "#Handle numeric missing values\n",
    "data[numeric_cols] = imputer.fit_transform(data[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode categorical features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    data[col] = encoder.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum().any() #Check if missing value(s) exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LotArea          12.822431\n",
       "KitchenAbvGr      4.302254\n",
       "BsmtFinSF2        4.146034\n",
       "EnclosedPorch     4.003891\n",
       "ScreenPorch       3.946694\n",
       "BsmtHalfBath      3.931148\n",
       "MasVnrArea        2.602112\n",
       "OpenPorchSF       2.535114\n",
       "WoodDeckSF        1.842433\n",
       "LotFrontage       1.563371\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling skewed features using log transformation\n",
    "skew_features = np.abs(data[numeric_cols].apply(lambda x: skew(x)).sort_values(ascending=False))\n",
    "skew_features[:10] # Displaying top ten skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering skewed features.\n",
    "high_skew = skew_features[skew_features > 1]\n",
    "# Taking indexes of high skew.\n",
    "skew_index = high_skew.index\n",
    "#Applying log transformation\n",
    "for i in skew_index:\n",
    "    data[i] = np.log1p(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new features  based on previous observations...\n",
    "data['TotalSF'] = data['BsmtFinSF1'] + data['BsmtFinSF2'] + data['1stFlrSF'] + data['2ndFlrSF']\n",
    "data['TotalBathrooms'] = data['FullBath'] + (0.5*data['HalfBath']) + data['BsmtFullBath'] + (0.5*data['BsmtHalfBath'])\n",
    "data['TotalPorchSF'] = data['OpenPorchSF'] +  data['EnclosedPorch'] + data['ScreenPorch'] + data['WoodDeckSF']\n",
    "data['YearBlRm'] = data['YearBuilt'] + data['YearRemodAdd']\n",
    "\n",
    "# Merging quality and conditions.\n",
    "data['TotalExtQual'] = data['ExterQual'] + data['ExterCond']\n",
    "data['TotalBsmQual'] = data['BsmtQual'] + data['BsmtCond'] + data['BsmtFinType1'] + data['BsmtFinType2']\n",
    "data['TotalGrgQual'] = data['GarageQual'] + data['GarageCond']\n",
    "data['TotalQual'] = data['OverallQual'] + data['TotalExtQual'] + data['TotalBsmQual'] + data['TotalGrgQual'] + data['KitchenQual'] + data['HeatingQC']\n",
    "\n",
    "# Creating new features by using new quality indicators.\n",
    "data['QualGr'] = data['TotalQual'] * data['GrLivArea']\n",
    "data['QualBsm'] = data['TotalBsmQual'] * (data['BsmtFinSF1'] + data['BsmtFinSF2'])\n",
    "data['QualPorch'] = data['TotalExtQual'] * data['TotalPorchSF']\n",
    "data['QualExt'] = data['TotalExtQual'] * data['MasVnrArea']\n",
    "data['QualGrg'] = data['TotalGrgQual'] * data['GarageArea']\n",
    "data['QlLivArea'] = (data['GrLivArea']  * data['TotalQual'])\n",
    "data['QualSFNg'] = data['QualGr'] * data['Neighborhood']\n",
    "\n",
    "#create binary columns\n",
    "binary_column = ['2ndFlrSF', 'QualGrg', 'Fireplaces', 'QualBsm', 'QualPorch','TotalPorchSF']\n",
    "for col in binary_column:\n",
    "    col_name = 'has_'+ col\n",
    "    data[col_name] = data[col].apply(lambda x: 1 if x > 0 else 0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:1460,:]\n",
    "X_test = data.iloc[1460:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# Scale the dataset\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "cols = X.select_dtypes(np.number).columns\n",
    "scaler = RobustScaler().fit(X[cols])\n",
    "X[cols] = scaler.transform(X[cols])\n",
    "X_test[cols] = scaler.transform(X_test[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are hyperparameters?\n",
    "Hyperparameters are settings that can be adjusted to improve the overall predictive performance of our model. It can be *colsample_bytree* in XGBoost or *min_samples_leaf* in RandomForest.\n",
    "Most times, every model has different parameter, so you will have to read up on the parameter of the model you want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is hyperparameter optimization?\n",
    "Hyperparameter tuing or optimization is the process of finding the best hyperparameters, which can results in performance improvement. In nutshell, we want to find the optimal set of parameters.\n",
    "Hyperparameter tuning is an important concept in modelling exercise and it is worth doing well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook  will use XGBoost as the baselilne model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_model = XGBRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every model provides a way to get the parameters name and its default value. The parameters and its default value can be obtained as a python dictionary using **.get_params()**. In xgboost, this can be achieved using **.get_params()** or **.get_xgb_params()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'importance_type': 'gain',\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': 1,\n",
       " 'nthread': None,\n",
       " 'objective': 'reg:linear',\n",
       " 'random_state': 42,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': None,\n",
       " 'silent': None,\n",
       " 'subsample': 1,\n",
       " 'verbosity': 1}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation with default parameter default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:21:15] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(random_state=42)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16401.73058468893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "xgb_preds = xgb_model.predict(X_val)\n",
    "print(mean_absolute_error(y_val, xgb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You will have to read up the documentation to understand what a parameter does. For example, these are parameters of XGBoost and their function*\n",
    "\n",
    "\n",
    "* **booster:** Select the type of model to run at each iteration\n",
    "    * gbtree: tree-based models\n",
    "    * gblinear: linear models\n",
    "* **nthread:** default to maximum number of threads available if not set\n",
    "* **objective:** This defines the loss function to be minimized\n",
    "\n",
    "**Parameters for controlling speed**\n",
    "\n",
    "* **subsample:** Denotes the fraction of observations to be randomly samples for each tree\n",
    "* **colsample_bytree:** Subsample ratio of columns when constructing each tree.\n",
    "* **n_estimators:**  Number of trees to fit.\n",
    "\n",
    "**Important parameters which control overfiting**\n",
    "\n",
    "* **learning_rate:** Makes the model more robust by shrinking the weights on each step\n",
    "* **max_depth:** The maximum depth of a tree.\n",
    "* **min_child_weight:** Defines the minimum sum of weights of all observations required in a child."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Techniques of hyperparameter optimization?**\n",
    "* Manual Search\n",
    "* Grid Search\n",
    "* Randomized Search\n",
    "* Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's address each techniques one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual Search\n",
    "In manual search, we change value of the parameter, train the model, check the score for improvement, without automating the process of selecting the parameters to change. This is done iteratively until our best performing parameters are gotten. \n",
    "This method is completely manual & tedious process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:21:19] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "16027.002630921803\n"
     ]
    }
   ],
   "source": [
    "#Let's try to change values of some parameters\n",
    "xgb_model_m_tuned = XGBRegressor(random_state=42, n_estimators=500)\n",
    "xgb_model_m_tuned.fit(X_train, y_train) \n",
    "\n",
    "xgb_preds_m_tuned = xgb_model_m_tuned.predict(X_val)\n",
    "print(mean_absolute_error(y_val, xgb_preds_m_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by changing the number of estimator from 100 to 500, the mean absolute error decreased from 164071 to 16027. Let's change the the subsample from 1 to 0.5 to observe the influence on the model's error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:21:41] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "15670.421848244863\n"
     ]
    }
   ],
   "source": [
    "#Let's try to change values of some parameters\n",
    "xgb_model_m_tuned2 = XGBRegressor(random_state=42, n_estimators=500, subsample=0.5)\n",
    "xgb_model_m_tuned2.fit(X_train, y_train) \n",
    "xgb_preds_m_tuned2 = xgb_model_m_tuned2.predict(X_val)\n",
    "print(mean_absolute_error(y_val, xgb_preds_m_tuned2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search\n",
    "The approach of grid search is quite simple, it's a brute-force exhaustive search method where we specify a list of values for different hyperparameters, and the computer evaluates the model performance for each combination of those to obtain the optimal set. We perform GridSearch by initializing a GridSearchCV object from the sklearn.grid_search module, and then train and tune on our model or pipeline.\n",
    "In grid search, you can cover all possible prospective sets of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB**: If it an hyperparameter is not specified in your grid, it will never be tried during the search phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_tuning = {\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators' : [100, 200, 500, 1000],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If you want to you can uncomment it.\n",
    "# grid_search = GridSearchCV(estimator = xgb_model,\n",
    "#                        param_grid = param_tuning,\n",
    "#                        cv = 5,\n",
    "#                        n_jobs = -1,\n",
    "#                        verbose = 1)\n",
    "\n",
    "\n",
    "# grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Get the best values for specified parameters.\n",
    "# grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace xgboost default parameter value with the value gotten from grid_search,\n",
    "# then train your model again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although grid search is a powerful approach for finding the optimal set of parameters, the evaluation of all possible parameter combinations is also computationally very expensive.\n",
    "The computation expensiveness serves as a major drawback.\n",
    "As the number of hyperparameterizations increases, processing times also increases and becomes slower. Also, during the process of defining our hyperparameters, we may omit an hyperparameterization that would in fact be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Randomized Search \n",
    "An alternative approach to sampling different parameter combinations using scikit-learn is randomized search. Using the RandomizedSearchCV class in scikit-learn, we can draw random parameter combinations from sampling distributions with a specified budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our set of hyperparameter to tune just like gridsearch, but the hyperparameter set is\n",
    "randomly selected from prepared hyperparameters search space already defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random search is based on sampling from distributions of hyperparameters, which allows you to expand the search range, thereby giving you the chance to discover a good solution that you may miss with the grid or manual search options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_tuning2 = {\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'n_estimators' : [100, 200, 500, 1000]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If you want to you can uncomment it.\n",
    "# random_search = RandomizedSearchCV(estimator = xgb_model,\n",
    "#                        param_distributions = param_tuning2,\n",
    "#                        cv = 3,\n",
    "#                        n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the best values for specified parameters.\n",
    "# random_search.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace xgboost default parameter value with the value gotten from grid_search,\n",
    "# then train your model again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In randomized search, there is probability that your some important search space may not be explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian optimization, it starts from random search of the seaech space, and then narrow the search space based on Bayesian approach. The popular libraries for bayesian optimization are scikit-optimize(skopt) and hyperopt. You will have to check the resources online for its implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
